{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "random_forest_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MRC0e0KhQ0S"
      },
      "source": [
        "# Random Forest Classification\n",
        "Random Forest Classification is essentially  multiple decision trees working together (hence random FOREST).This dataset contains user data and website clickstreams of customers from a online store and whether or they bought an item. This model is able to predict about 90% of customer decisions on buying based on their time online.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWd1UlMnhT2s"
      },
      "source": [
        "### Importing the libraries\n",
        "These are the three go to libraries for most ML.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvGPUQaHhXfL"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1VMqkGvhc3-"
      },
      "source": [
        "### Importing the dataset\n",
        "I imported the dataset through Pandas dataframe and used iloc to assign the variables. Remember that the name of the dataset has to be updated for diff usecases AND it must be in the same folder as your .py file or uploaded on Jupyter Notebooks or Google Collab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M52QDmyzhh9s"
      },
      "source": [
        "dataset = pd.read_csv('online_shoppers_intention (1).csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "garmncPti9lL"
      },
      "source": [
        "### Encoding categorical data\n",
        "Index 15,16,10 had categorical data that had to be converted using OneHotEncoding.This must be done AFTER imputing missing values since OneHotEncoding automatically makes the encoded column the first index which displaces all the rest.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQIXUJYTO41d"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [15,16]), ('encoder2', OneHotEncoder(), [10])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvxIPVyMhmKp"
      },
      "source": [
        "### Splitting the dataset into the Training set and Test set\n",
        "Because of the large dataset and many variables for a simple algorothim I used a 80/20 split. The random state is tuned to 0 for consistency sakes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVzJWAXIhxoC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW3c7UYih0hT"
      },
      "source": [
        "### Feature Scaling\n",
        "We can feature scale our data to make it easier for our model to train on our data and give us accurate results that aren't shifted by the presence of extreme outliers. It's not necessary, but helpful for getting more accurate results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fQlDPKCh8sc"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb6jCOCQiAmP"
      },
      "source": [
        "### Training the Random Forest  model on the whole dataset\n",
        "Here, 'n_estimators' is the value assigned to how many trees we have. 10 is usually the go to but with model tweaking and grid search and k fold, you can find the optimal amount for your specific problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0pFVAmciHQs",
        "outputId": "069f41d1-7d3c-4db3-d074-03feed43cbdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators = 15, criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=15,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKYVQH-l5NpE"
      },
      "source": [
        "### Predicting the Test set results\n",
        "By using the concatenate function I display the predicted values and  actual values in a side by side 2D array through '(len(y_wtv), 1))' for easy viewing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6VMTb2O4hwM",
        "outputId": "c1901f56-b95d-4f93-c9f9-5edf5efdcdf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[False False]\n",
            " [False False]\n",
            " [False False]\n",
            " ...\n",
            " [False False]\n",
            " [False False]\n",
            " [ True  True]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Hwj34ziWQW"
      },
      "source": [
        "### Making the Confusion Matrix\n",
        "The confusion matrix is a useful metric for classification models to allow us to visualize the correct positive, false positive, false negative, and correct negative predictions as well as a ultimate accuracy score on the bottom. Our accuracy score for this model is nearly 90%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6bpZwUiiXic",
        "outputId": "dab49d7f-6ebe-41a3-949e-ae75ab295588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1956   88]\n",
            " [ 182  240]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8905109489051095"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}